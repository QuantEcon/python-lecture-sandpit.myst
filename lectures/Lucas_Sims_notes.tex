


\index{Lucas Critique}%
\section{The Lucas Critique}\label{sec:lucascritique}%
 We can use our chapter \use{timeseries} tools concisely to describe reasoning that led Robert E. Lucas, Jr., (1976) to criticize a class of  procedures that were widely used in applied work at the
 time that he wrote, and that occasionally still are.
Assume a jointly stationary Gaussian stochastic process for $\{x_t, y_t, d_t\}_{t=0}^\infty$ generated by   a state-space system
$$ \EQNalign{ x_{t+1} & = A x_t + C w_{t+1}  \EQN lucas101;a \cr
             y_t & = G x_t \EQN lucas101;b \cr
             d_t & = S x_t \EQN lucas101;c \cr
             S & = H G (I - \delta A)^{-1}  \EQN lucas101;d \cr} $$
where $\delta \in (0,1)$ is a discount factor and $A$ is a stable matrix
  that in applications itself often depends on deeper parameters,
   and   $x_0 \sim {\cal N}(\mu, \Sigma) $, where $\mu, \Sigma$ are the mean vector and covariance matrix, respectively, of a stationary distribution for the
   $\{x_t\}_{t=0}^\infty$ process.  As usual, $w_{t+1}$ is the time $t+1$ component of   an i.i.d.~sequence of ${\cal N}(0,I)$ random vectors.
   Here $d_t$ is a vector of decisions that  an econometrician observes. Equations \Ep{lucas101;c} and \Ep{lucas101;d} assert that $d_t$ equals a  fixed matrix $H$ times a   vector of
forecasts of       discounted sums of current and future $y_t$ vectors, forecasts that equal  appropriate conditional expectations
   (please see equation \Ep{discount1}). To fix ideas with an example, take a univariate autoregression for $\{y_t\}$, namely,
$$ y_{t+1} = \alpha + \rho_1 y_t + \rho_2 y_{t-2} + \sigma w_{t+1}  \EQN lucas102 $$
 that we can represent with equation \Ep{lucas101;a}-\Ep{lucas101;b}.  We can  think of
$d_t$ as a time $t$ {\it decision\/}  and $d_t = S x_t$ as a {\it decision rule\/} that tells how an  economic agent or a collection  of economic agents sets
$d_t$ as a function of time $t$ information $x_t$ about future $y_{t+j}$'s, in particular, so that
$d_t = H E_t \sum_{j=0}^\infty \delta^j y_{t+j}$ where $E_t$ denotes a mathematical expectation conditional on time $t$ information.
Lucas (1976) offered three examples that can be  fit into this framework. In those examples the decisions $d_t$ were
 choices either  of a consumption rate or  an   investment rate 
or a labor supply (i.e., Lucas's Phillips curve example), and $y_t$ was a stochastic process embedding 
government polices 
(e.g., a tax rate or inflation process), forecasts
of which influenced time $t$ decisions $d_t$.\NFootnote{A variety of other examples  that we'll
encounter later in this book also fit. Also see Hansen and Sargent (1980, 1981). In those examples, counterparts of the parameter $\delta$
in equation  \Ep{lucas101;d} are themselves functions of deeper parameters measuring costs of adjustment and other features of
technologies and preferences.}
\auth{Lucas, Robert E., Jr.}%


Note that  by virtue of stationarity, $\bmatrix{d_t \cr x_t}$ is a  multivariate normal vector  with partitioned covariance matrix
$\bmatrix{  \Sigma_{dd} & S\Sigma_{dx} \cr \Sigma_{xd} S' & \Sigma_{xx}}$, where $\Sigma_{xx}$ satisfies  the discrete Lyapunov equation $\Sigma_{xx}
= A \Sigma_{xx} A' + C C'$ and $\Sigma_{dx} = S \Sigma_{xx}$.  Application of the least squares formula \Ep{leastsq12} confirms that  $S$  equals a matrix of population regression coefficients of $d_t$ on $x_t$
   and that the $R^2$ for each component of $d_t$ is $1$.\NFootnote{Apply formula \Ep{leastsq12} for the matrix of regression coefficients
    and notice that $\beta = S \Sigma_{xx} \Sigma_{xx}^{-1} = S$
   and that the conditional covariance matrix $\hat \Sigma_{dd} = 0$.}

   %
%
%
%under the assumption
%that the researcher does not understand the origin of the decision rule $S$ via restriction \Ep{lucas101;d}. Instead,
%the researcher has a large set of observations on $\{d_t, x_t\}$ and estimates $S$ by least squares regression of $d_t$ on $x_t$.

Lucas (1976) criticized a collection of  studies that he said had used the following faulty logic.

\medskip

\item{a.}  A researcher   just wants to know  the decision rule for $d_t$  and ignores that   
 $S$ satisfies restriction \Ep{lucas101;d}.

\medskip

\item{b.}  The researcher gathers   historical observations on $\{d_t, x_t\}_{t=0}^T$,
 correctly believes that $d_t = S x_t$ is a
regression equation, and therefore  estimates the matrix  $S$ of regression coefficients by applying  least squares  equation-by-equation.

\medskip

\item{c.}  Mistakenly as it will turn out, the researcher  assumes that  the  decision rule $d_t = S x_t$ that he has estimated
 from historical data  will remain  invariant under
 arbitrary hypothetical alterations  of the right-hand side variables $\{x_t\}$, including ones that   would
 deviate from the historical law of motion
 \Ep{lucas101;a}-\Ep{lucas101;b}.    He falsely believes that he can use that estimated  decision rule to predict how
 variations in $\{x_t\}$ that systematically depart from system  \Ep{lucas101;a}-\Ep{lucas101;b} will induce
  systematic variations in  
  $\{d_t\}$.\NFootnote{Of course, he could use the estimated decision rule to predict the consequences of  variations in $\{x_t\}$ that
  {\it are \/} described by  system \Ep{lucas101;a}-\Ep{lucas101;b}.}

\medskip

\item{d.} Holding the historically estimated decision rule $S $ fixed,
 the researcher simulates outcomes $\{y_t, d_t\}_{t=T+1}^{\tilde T}$
under alternative rules for generating
 time paths  for
 $\{x_t\}_{t=T+1}^{\tilde T}$ and then evaluates the
resulting $\{y_t, d_t, x_t\}_{t=T+1}^{\tilde T}$
outcomes according to a social  welfare function that depends on these outcome paths.   %, that are functions of the joint sequence $\{d_t, x_t\}$.


\medskip

\item{e.} After
learning how the  welfare function varies with the hypothetical rule for setting the $\{x_t\}_{t=T+1}^{\tilde T}$ sequence,
the researcher recommends  a continuation $\{x_t\}_{t=T+1}^{\tilde T}$
sequence that yields the best social welfare. For example, the researcher holds $S$ fix and uses it as a component
of a state-space system to which he applies a linear-quadratic optimal control theory to be described in chapter \use{dplinear} to  recommend a
law of motion for $x_t$ that manifests itself as a new pair of matrices $\tilde A, \tilde G$  that in the future
will replace $A, G$ in the state-space representation \Ep{lucas101}, while he holds $S$ fixed at the value estimated from historical data.

\medskip

\item{f.} When  the  decision makers who choose $d_t$ recognize that under the new government policy  $\tilde A, \tilde G$ and not $A, G$ will describe future $y_t$'s, they adjust the decision rule $S$  to become 
 $\tilde S = H \tilde G (I - \delta \tilde A)^{-1}$. By failing to understand that,
the researcher and policy maker will be disappointed to find that the regression  that the researcher  estimated on the historical data  fails to  describe data generated
under the regime $\tilde G, \tilde A$.

\medskip

Lucas (1976) said that the  line of reasoning in points (a)-(e)    characterized an influential  1940s-1960s econometric policy evaluation
research method that  applied optimal control techniques to Keynesian and monetarist
econometric models to propose  improved  macroeconomic policies. 
In point (f) Lucas said that the method was flawed because it  ignored
 the key cross-equation restrictions \Ep{lucas101;d}. Altering the policy-generating
matrices from the $G, A$ pair that prevailed historically to new pair  $\tilde G, \tilde A$ that describes a  hypothetical policy ``experiment'' that we want to study
  amounts to a  positing a  ``regime change'' that
 should be expected  systematically to alter how private decision makers make  $y_t$ depend on $x_t$, as
 described by
 % that would alter   the $A,G$ pair %and the associated $x_t$
%of matrices that appears
%in
 system \Ep{lucas101}.  % and that governs the $\{y_t, x_t\}$ process.
 A
 change in the decision maker's  environment $G, A$   alters the moment matrices
$ \Sigma_{xx}, \Sigma_{dx}$ and therefore the population regression matrix of $d_t$ on $x_t$ that should be 
expected to characterize outcomes
under the new  regime. Thus,  structure \Ep{lucas101}, and in particular the cross-equation restrictions \Ep{lucas101;d},
imply that $S$ will vary {\it systematically\/} with  hypothetical changes
in  $\tilde G, \tilde A$, belying the assumption that $S$ is invariant to the hypothetical policy interventions being investigated.
A concise way to say this is that the joint probability distribution of the $\{d_t, x_t\}$ process and the implied
  probability distribution
of $d_t$ conditional on  $x_t$ process both  depend on the $(A, C, G)$ triple of matrices.

Lucas (1976) summarized his argument as follows:

\epigraph{$\ldots$ given that the structure of an econometric model consists of optimal decision rules of economic agents, and
that optimal decision rules vary systematically with changes in the structure of series relevant to the decision makers, it follows
that any change in policies will systematically affect the structure of econometric models.}{``Econometric Policy Evaluation: A Critique,'' 1976}


 Equation \Ep{lucas101;d} is an effective way of capturing  how ``optimal decision rules vary systematically with changes in the structure of
 series relevant to the decision makers.''  We already  encountered an example of  such restrictions  on optimal decision rules in
 section \use{sec:LQmodel} when we found that the LQ permanent income model of consumption smoothing implies an optimal decision rule
   for consumption of the form \Ep{eqn:lucascritiquecons}. We shall encounter  more examples  in chapter \use{dplinear}.
  We can read   Lucas (1976) as  calling for  a rational expectations econometrics that incorporates  the kinds of cross-equation
restrictions  that system \Ep{lucas101} encodes as an essential part of a better theory of econometric policy 
evaluation.\NFootnote{See  Sargent (1981) and the introductory essay in Lucas and Sargent (1981), a volume containing
 early papers on  rational expectations econometrics.  See  Hansen and Sargent (1991a) for
further
technical contributions.}
\index{rational expectations!econometrics}
\auth{Lucas, Robert E., Jr.}%
\auth{Hansen, Lars P.}%
\auth{Sargent, Thomas J.}%


\section{Responses to the Lucas Critique}\label{sec:lucascritique2}%
Section \use{sec:lucascritique} already mentioned a principal response to the Lucas Critique,  namely,
rational expectations econometrics, an estimation and interpretation strategy now widely used throughout  applied economics.
In this section, we use a  state-space system with coefficients that are functions of a finite state Markov process 
as a vehicle to describe
another response
%We describe what amount to either extensions to, or  alternatives to, or elaborations of rational expectations econometrics
that takes the perspective that 
changes in  government  decision rules  or ``regimes''   
unfold in ways that private agents  anticipate.  %don't occur in a vacuum but were
%understood during the historical
%sample period being modeled econometrically.
%\NFootnote{Section 3 of Lucas (1976) suggested that  longstanding  evidence that coefficients in
%macroeconomic models drifted over time reflected   the workings  of  cross-equation restrictions  
%like \Ep{lucas101;d}.} 
Sargent and Wallace (1976) argued that


\epigraph{$\ldots$ new rules are not adopted
in a vacuum. Something would cause the change -- a change in adminstrations,
new appointments, and so on. Moreover, if rational agents live in a world in
which rules can be and are changed, their behavior should take into account such
possibilities and should depend on the process generating the rule changes. But
invoking this kind of complete rationality seems to rule out normative economics
completely by, in effect, ruling out freedom for the policymaker. For in a model
with completely rational expectations, including a rich enough description
of policy, it seems impossible to define a sense in which there is any scope for
discussing the optimal design of policy rules. That is because the equilibrium
values of the endogenous variables already reflect, in the proper way, the
parameters
describing the authorities' prospective subsequent behavior, including
the probability that this or that proposal for reforming policy will be adopted.}{``Rational Expectations and the Theory of Economic Policy,''
Sargent and Wallace (1976), p.~181}


\auth{Wallace, Neil}%
\auth{Sargent, Thomas J.}%

This view sees  government decision rules as  statistical processes
 that are known to the private agents living inside a rational expectations model.  
 So  those private agents properly anticipate changes in government decision rules.
 Sargent and Wallace's   view that  government policy rules  are
determined by a well understood process  leaves  econometricians and policy analysts with
 no ``free parameters'' to change,
disabling them from being able to offer policy advice.
This perspective  was embraced  by
 Christopher A. Sims (1982) and Sargent (1984).\NFootnote{Musto and Yilmaz (2003) extended the approach 
 to a political 
 economy  with complete financial markets.}
 \auth{Musto, David K.}%
 \auth{Yilmaz, Bilge}%
 
\auth{Sims, Christopher A.}%
\auth{Zha, Tao}%

In describing  vector autoregressions with regime shifting coefficients and volatilities, Sims and Zha (2006)  link
their work to the Lucas Critique: %\example
\epigraph
{The model with time variation in coefficients in all equations might be expected to fit best if there 
were policy regime changes, and the nonlinear effects of these changes on private sector dynamics, via
 changes in private sector forecasting behavior, were important. That this is possible was the main point 
 of Robert E. Lucas (1972).
$\ldots$ % as Sims (1987) has explained at more length,
once we recognize that changes in policy must in principle themselves be modeled as 
stochastic, Lucas's argument can be seen as a claim that a certain sort of nonlinearity is important. Even if the public believes that policy is time-varying and tries to adjust its expectation formation accordingly, its behavior could be well approximated as linear and non-time-varying. As with any use of a linear approximation, it is an empirical matter whether the linear approximation is adequate for a particular sample or counterfactual analysis.}{``Were There Regime  Switches in U.S. Monetary Policy?'' Sims and Zha (2006), p.~59}
%\endexample


\noindent Here we'll use chapter \use{timeseries} tools to  study these parts of Sims and Zha's analysis:

\medskip
\item{a.} How to represent ``nonlinearities'' in the form of dependencies of linear state-space model 
coefficients that are functions of  a discrete  Markov state variable that
stands in for a ``regime''.

\medskip
\item{b.} How agents who understand regime transition probabilities and who recognize the current regime shape their decisions.

\medskip
\item{c.} How a best-fitting fixed-coefficient linear model that ignores regime changes can approximate observed 
outcomes.


\medskip

\subsection{Markov switching linear state space model}

We  can  represent the regime-change-induced 
nonlinearities mentioned by Sims and Zha policy  with  a linear state space model with
 coefficients that are governed by a finite state Markov chain.\NFootnote{Hamilton (1989) introduces this 
  type of model of a stochastic process  and labels it  as ``non-stationary''.  If  we assume that 
chain $\pi_0$ is  a stationary distribution, then the model  actually generates a stationary stochastic process,
 a property that we exploit  below. Williams and Svensson (2008) and Svensson and Williams (2009) used linear quadratic dynamic programming problems with Markov-switching coefficients to model optimal
monetary policies. }
Let $s_t \in \{1,2\}$ be governed by a Markov chain with transition matrix $P$, where
$P_{ij} = {\rm Prob}(s_{t+1} = j| s_t  = i) $, $\pi_{0,i} = {\rm Prob}(s_0 = i)$, and  $\pi_{t,i} = {\rm Prob}(s_t = i)$.
We assume that $\{x_{t}, y_t\}_{t=0}^\infty$ is governed by  the linear state-space system with  
 Markov-switching matrices  
$$ \eqalign{ x_{t+1} & = A_{s_t} x_{t+1} + C_{s_t} w_{t+1} \cr
                    y_t  & = G_{s_t} x_t  \cr  }   \EQN MSLQ $$ %
where $x_0 $ is a random initial vector drawn from  density  $\phi_0$ and $w_{t+1} \sim {\cal N}(0,I)$ is an i.i.d.~sequence of random
vectors.
We   want to compute the conditional  mathematical expectations
 $$ z_t = E ( \sum_{j=0}^\infty \delta^t y_{t+j} ) \Bigl| x_t, s_t  $$
  and
 $$ \tilde z_t =  E ( \sum_{j=0}^\infty \delta^t y_{t+j} ) \Bigl| x_t =  E z_t | x_t , $$
 where the last equality follows by an application of the \idx{law of iterated expectations}.
 We also want to compute an associated  time-invariant vector autoregression that is implied by   \Ep{MSLQ} and that takes the form
  $$ x_{t+1} = \bar A x_t + v_{t+1} \EQN VARtimeinv$$
  where $v_{t+1} $ is a serially uncorrelated shock with mean zero and contemporaneous covariance matrix $E v_{t+1} v_{t+1}' = \bar \Sigma$.
This is the object that Sims and Zha mention when they say of the time-varying system that ``its behavior could be well approximated as linear and non-time-varying''.
\auth{Hamilton, James D.}%
\auth{Williams, Noah}%
\auth{Svensson, Lars E. O.}%
\auth{Sims, Christopher A.}%
\auth{Zha, Tao}%



\subsection{Example}
Before computing these objects, let's take an example fashioned after a rational expectations version of Philip Cagan's (1956) model of
hyperinflation.   The rate of inflation is the first difference of the logarithm of the price level
and the rate of growth of the money supply is  the first difference of the logarithm of the money supply.
Let $p_t$ be the rate  of inflation and let $m_t$ be the rate of growth of the money supply.  Assume
that
$$ p_t = (1-\lambda) m_t + \lambda E_t p_{t+1},  \quad \lambda \in (0,1)  $$
so that
$$ p_t = (1-\lambda) E_t \sum_{j=0}^\infty \lambda^j m_{t+j} , $$
where $E_t (\cdot)$ is a mathematical expectation conditioned on time $t$ information, which will be either $x_t$ or $(x_t, s_t)$, where the pair $(x_t, s_t)$ is defined as follows.
We assume  that $s_t \in {1,2}$ is governed by a two-state Markov chain $P$ with initial distribution $\pi_0$.
Setting  $x_t= \bmatrix{1 & m_t & m_{t-1} }'$   lets us capture the following stochastic process for $\{m_t\}_{t=0}^\infty$ with the state-space form \Ep{MSLQ }:
$$ m_{t+1} = \rho_{1,s_t} m_t + \rho_{2,s_t} m_{t-1} + c_{s_t}w_{t+1}, \EQN second_order_AR $$
where $w_{t+1}$ is an i.i.d.~univariate standardized normal random variable. In subsection \use{sec:cond_dist_102},
we'll use this example
as a vehicle for illustrating Sims and Zha's remark about how  a linear time-invariant autoregression can approximate
a Markov-switching autoregression like \Ep{second_order_AR}.
\auth{Cagan, Phillip}%

\subsection{Conditional distributions of geometric sums}
We begin by computing  $z_t$, an expectation conditional on both $x_t$ and $s_t$.
We begin by guessing that
$$ z_t = H_{s_t} x_t  $$
and substitute this guess into
$$ z_t = y_t + \delta  E  [ z_{t+1} | x_t, s_t ] $$
to get
$$  H_i  = G_{i}   + \delta \sum_{j} H_j A_i P_{ij}, $$
which implies  that
$$ \eqalign{ H_1 & = G_1 + \delta [ H_1 A_1 P_{11} + H_2 A_1 P_{12} ] \cr
                   H_2 & = G_2 + \delta [ H_1 A_2 P_{21} + H_2 A_1 P_{22} ]. }$$
Stacking these equations and solving for the matrices $H_1, H_2$, we compute
$$ \bmatrix{H_1 & H_2 }  = \bmatrix{G_1 & G_2}             (I - \delta \bmatrix {A_1 P_{11} & A_2 P_{21} \cr
                                                   A_1 P_{12} & A_2 P_{22} }  )^{-1}. $$



Next we compute $ \tilde z_t =  E ( \sum_{j=0}^\infty \delta^t y_{t+j} ) | x_t$.
%We let $\pi_t$ be the distribution over the finite Markov state $s_t$. Note that the
 We start knowing that the probability distribution of $x_{t+1}$  conditional on $(x_t, s_t)$ is multivariate normal:
$$  x_{t+1} | x_t, s_t   \sim {\cal N}( A_{s_t} x_t, C_{s_t}C_{s_t} ' ) . $$
Distributions of $x_{t+1}, y_t$, and $z_t$ conditional on $x_t$ are {\it mixtures of normals}, with $\pi_{t,i}$   being the mixing probabilities over states $i$  at time $t$.
Defining the matrix averages
$$ \eqalign{ \bar A_t & = \pi_{t,1} A_1 + \pi_{t,2} A_2   \cr
                    \bar G_t & =\pi_{t,1} G_1 + \pi_{t,2} G_2 \cr
                    \bar H_t & = \pi_{t,1} H_1 + \pi_{t,2} H_2  ,}$$
we can deduce that means  conditional on $x_t$ are 
$$ \eqalign{ E x_{t+1} | x_t & = \bar A_t x_t \cr
                    E y_t | x_t & = \bar G_t x_t      \cr
                    E z_t | x_t & \equiv \tilde z_t = \bar H_t x_t   . }$$
The appearance of  time subscripts $t$ in the formulas for $\bar A_t, \bar G_t, \bar H_t$ are a 
consequence of  $\pi_t$ not necessarily
being a stationary distribution for $s_t$. % for Markov chain $P$.

\subsection{More conditional distributions}\label{sec:cond_dist_102}%
Having computed conditional means,  let's compute covariances conditional on $x_t$.
Since
$$ x_{t+1} - \bar A_t x_t = \cases{ (A_1 - \bar A_t) x_t  + C_1 w_{t+1} & if $s_t =1$ \cr
                                                    (A_2 - \bar A_t) x_t  + C_2 w_{t+1} & if $s_t =2$ , } $$
it follows that
$$ \EQNalign{ E (x_{t+1} - \bar A_t x_t ) (x_{t+1} - \bar A _tx_t ) ' | x_t & = \sum_i \pi_{t,i} [ (A_i - \bar A_t) x_t x_t' (A_i - \bar A_t)' + C_i C_i' ] \cr
        & \equiv \Sigma_t(x_t) \EQN Sigmatxt }   $$
where $i$ is present through $\pi_{t,i}$ and $x_t$ is present because it appears on the right side of the first line.
Since
$$ y_t = \cases{ G_1 x_t & if $s_t =1$ \cr
                           G_2 x_t & if $s_t =2 $ } $$
and
$$ y_t - \bar G_t x_t = \cases{ (G_1 - \bar G_t) x_t & if $s_t =1$ \cr
                           (G_2  - \bar G_t) x_t & if $s_t =2 $ , } $$
it follows that
$$ E (y_t - \bar G_t x_t) (y_t - \bar G_t x_t)' | x_t = \sum_i \pi_{t,i} 
(G_i - \bar G_t) x_t x_t' (G_i - \bar G_t)' \equiv \Sigma_{y,t}(x_t)   $$
and similarly that
$$ E (z_t - \bar H_t x_t) (z_t - \bar H_t x_t)' | x_t = \sum_i \pi_{t,i} (H_i - \bar H_t) x_t x_t'
 (H_i - \bar H_t)' \equiv \Sigma_{z,t}(x_t),  $$
 where the presence of $t$ subscripts in $ \Sigma_{y,t}(x_t)$ and $\Sigma_{z,t}(x_t)$ reflects that
 $\pi_t$ might not be a stationary distribution, so that it and $\bar G_t$ and $\bar H_t$ all depend on $t$.
We can compute  unconditional means and covariances of the $\{x_t\}_{t=0}^\infty$  process as follows.
Define the unconditional moments
$E x_t = \mu_t$ and $\Sigma_t = E (x_t - \mu_t) (x_t  - \mu_t)'$.
Note that
$$ x_{t+1} - \mu_{t+1} = \cases{ A_1 (x_t   - \mu_t)  + C_1 w_{t+1} & if $s_t =1$ \cr
                                                    A_2  (x_t   - \mu_t)  + C_2 w_{t+1} & if $s_t =2$ .} $$
It follows that
$$ \mu_{t+1} = \bar A_t \mu_t$$
and
$$\Sigma_{t+1} = \pi_{t,1} (A_1 - \bar A_t) \Sigma_t (A_1 - \bar A_t)' + \pi_{t,2} (A_2 - \bar A_t) \Sigma_t (A_2 - \bar A_t)' + (\bar C \bar C')_t  $$
where $(\bar C \bar C')_t = \sum_i \pi_{t,i} C_i C_i' $.
When  $\pi_t \rightarrow \bar \pi$ as $t$ goes to infinity, iterations on these two recursions  converge, respectively, to
a stationary mean vector $\mu$  and a stationary covariance matrix $\Sigma$ that
satisfy
$$ \mu  = \bar A \mu $$
and
$$\bar \Sigma =  \bar \pi_1 (A_1 - \bar A) \bar \Sigma (A_1 - \bar A)' + \bar \pi_2 (A_2 - \bar A) \bar \Sigma(A_2 - \bar A)' + \bar C \bar C' $$
where $\bar \pi_i = \lim_{t \rightarrow +\infty} \pi_{t,i}$ and $\bar C \bar C' = \sum_i \bar \pi_i C_i C_i'$.
A time-invariant vector autoregression of the form \Ep{VARtimeinv}, namely,
$$ x_{t+1} = \bar A x_t + v_{t+1} .$$
has $E v_{t+1} v_{t+1}' = \bar \Sigma$.
When $\pi_t = \bar \pi_t$, formulas for $\Sigma_y(x_t)$ and $\Sigma_z(x_t)$ also  simplify: 
$$ E (y_t - \bar G x_t) (y_t - \bar G x_t)' | x_t = \sum_i \pi_{i} 
(G_i - \bar G) x_t x_t' (G_i - \bar G)' \equiv \Sigma_{y}(x_t)   $$
and 
$$ E (z_t - \bar H x_t) (z_t - \bar H x_t)' | x_t = \sum_i \pi_{i} (H_i - \bar H) x_t x_t'
 (H_i - \bar H)' \equiv \Sigma_{z}(x_t),  $$
 where  $t$ subscripts have disappeared from  $ \Sigma_{t}(x_t)$ and $\Sigma_{z}(x_t)$ because 
 $\pi = \bar \pi$  and also now $\bar G$ and $\bar H$ no longer  depend on $t$.
 Under this stationary specification, we thus have the regression equations
 $$\eqalign{ x_{t+1} & = \bar A x_t + v_{t+1} \cr
            y_t & = \bar G x_t + \epsilon_{y,t} \cr 
             z_t & = \bar H x_t + \epsilon_{z,t}, }
$$
where $\bar H = \bar G (I - \delta \bar A)^{-1}$, $E v_{t+1} v_{t+1}'  = \bar \Sigma$,
 $E \epsilon_{y,t} \epsilon_{y,t}' = \Sigma_y(x_t), \ E \epsilon_{z,t} \epsilon_{z,t}' = \Sigma_z(x_t) $.


We can use some of the formulas above and our example in  equation \Ep{second_order_AR} to revisit Sims and Zha's (2006)
comments about how  a linear time invariant autoregression can approximate a Markov switching autoregression.
  We set $(\rho_{1,1}, \rho_{2,1}, c_{1}) =
(1.2, -.3, .5)$ and $(\rho_{1,2}, \rho_{2,2}, c_{2}) =
(.8, 0.0, .8)$, $P = \bmatrix{.5 & .5 \cr .5 & .5}$, and $\pi_0 = \bmatrix{.5 \cr .5} $, so that the one  step-ahead 
 conditional variance in state $s_t=1$  is $.25$  and in state $s_t=2$ is $.64$.  Applying the preceding formulas, we
can deduce the time-invariant second order autoregression
$$ m_{t+1} = 1.0 m_t - .15 m_{t-1} + v_{t+1} $$
where $E v_{t+1}^2 = \bar \Sigma = .46$, which is to be compared with the conditional variances $.25$ in state $s_t=1$ and
$.64$ in state $s_t = 2$.  
For this same example, it is instructive to set the discount factor $\delta =.95$ and set $G_i =
\bmatrix{1 & 0 \cr}'$ for $i = 1, 2$ so that $G_i$ does {\it \/} not depend on the Markov state $s_t$.  
In this case, we obtain 
$$ \bar H = \bar G (I - \delta \bar A)^{-1}  =  XXXX
 $$

% \medskip
% \noindent{\bf Request to Quentin:}  Could you please add to your notebook code for computing the quantities
% that required to compute $\bar H$.  The preceding can be computed 
% "analytically" using the above formulas, most of which you already have in the Jupyter notebook.
% Another things that could be "fun" to compute would be the {\it average\/} value of $\Sigma_z(x_t)$.
% I would recommend doing this just by running a very very long simulation and taking a time series average 
% of $(H_i - \bar H) x_t x_t'
% (H_i - \bar H)'$ as $s_t, x_t$ wander through time and states.  




\auth{Sims, Christopher A.}%
\auth{Zha, Tao}%



% \noindent{\bf Quentin R0:} Please go over the code in your notebook and reorganize to have all population computations at the front
% of the notebook. By ``population'' I mean analytic with our formulas, not the simulations with the least squares regressions. Please put that material at the end of the notebook. Please make the notebook so that Zejin can understand the flow and approve of its readability.

% \noindent{\bf Quentin R1:} Please compute and display conveniently for me the objects $C_1 C_1', C_2 C_2', \bar \Sigma, \bar A$.


% \noindent{\bf Quentin R2:} Please compute ``all of our objects'' for the above example with two $s_t$ dependent   $\rho_1, \rho_2$ vectors, namely $1.2, -.3$ and $.95, 0$ with
% a transition matrix $P = \bmatrix{ .5 & .5 \cr .5 & .5 } $ using as $\pi_0 =\pi_\infty$ the stationary distribution $\bmatrix{.5 & .5}'$
% or with another set of parameter values that I can input easily.  


% \medskip
% \noindent{\bf Quentin R3:} Please check whether $\bar H = \bar G (I -\delta \bar A)^{-1}$ and display outcomes so that Zejin and I can readily
% spot them.


\subsection{Time series averages}
Consider the conditional covariance matrix
$$\Sigma(x_t)  = \sum_i \pi_{t,i} [ (A_i - \bar A) x_t x_t' (A_i - \bar A)' + C_i C_i' ] .$$
Assume that the pair $(P, \pi_0)$ governing the  Markov state $s_t$ is stationary. Then  the joint process $\{x_t, s_t\}$ process is also  stationary,
and so is the  process $\{x_t\}$. Say that $x_t$ has  the unconditional probability distribution $\Phi(x)$.
Then a law of large numbers tells us that as $ {T \rightarrow + \infty}$
$$ T^{-1} \sum_{t=0}^T \Sigma(x_t)  \rightarrow \int \Sigma(x) d \Phi(x) \equiv \Sigma $$


