{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to regular packages contained in Anaconda by default, this notebook also requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import quandl as ql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Setup\n",
    "\n",
    "Let $X$ be an $m \\times n$ matrix of rank $r$.\n",
    "\n",
    "In this notebook, we'll think of $X$ as a matrix of **data**.\n",
    "\n",
    "  * each column is an **individual** -- a time period or person, depending on the application\n",
    "  \n",
    "  * each row is a **random variable** measuring an attribute of a time period or a person, depending on the application\n",
    "  \n",
    "  \n",
    "We'll be interested in  two distinct cases\n",
    "\n",
    "  * The **short and fat** case in which $m << n$, so that there are many more columns than rows.\n",
    "\n",
    "  * The  **tall and skinny** case in which $m >> n$, so that there are many more rows than columns. \n",
    "    \n",
    "   \n",
    "  \n",
    "We'll put a **singular value decomposition** of $X$ to work in both situations.\n",
    "\n",
    "In the first case in which there are many more observations $n$ than there are random variables $m$, we learn about the joint distribution of the  random variables by taking averages  across observations of functions of the observations. Here we'll look for **patterns** by using a **singular value decomosition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the second case in which there are many more random variables $m$ than observations $n$, we'll proceed in a different way. \n",
    "We'll again use a **singular value decomposition**,  but now to do a **dynamic mode decomposition** (DMD)\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "The **singular value decomposition** of an $m \\times n$ matrix $X$ of rank $r \\leq \\min(m,n)$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ X  = U \\Sigma V^T $$\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I  \\end{align*}\n",
    " \n",
    "where \n",
    " \n",
    " * $U$ is an $m \\times m$ matrix whose columns are eigenvectors of $X^T X$\n",
    " \n",
    " * $V$ is an $n \\times n$ matrix whose columns are eigenvectors of $X X^T$\n",
    " \n",
    " * $\\Sigma$ is an $m \\times r$ matrix in which the first $r$ places on its main diagonal are positive numbers $\\sigma_1, \\sigma_2, \\ldots, \\sigma_r$ called **singular values**; remaining entries of $\\Sigma$ are all zero\n",
    " \n",
    " * The $r$ singular values are square roots of the eigenvalues of the $m \\times m$ matrix  $X X^T$ and the $n \\times n$ matrix $X^T X$\n",
    " \n",
    " * When $U$ is a complex valued matrix, $U^T$ denotes the **conjugate-transpose** or **Hermitian-transpose** of $U$, meaning that \n",
    "$U_{ij}^T$ is the complex conjugate of $U_{ji}$. Similarly, when $V$ is a complex valued matrix, $V^T$ denotes the **conjugate-transpose** or **Hermitian-transpose** of $V$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of $U$, $\\Sigma$, and $V$ are $\\left(m, m\\right)$, $\\left(m, n\\right)$, $\\left(n, n\\right)$, respectively. \n",
    "\n",
    "Below, we shall assume these shapes.\n",
    "\n",
    "However, there is an alternative shape convention that we could have used, though we chose not to.\n",
    "\n",
    "Thus, note that because we assume that $A$ has rank $r$, there are only $r $ nonzero singular values, where $r=rank(A)\\leq\\min\\left(m, n\\right)$.  \n",
    "\n",
    "Therefore,  we could also write $U$, $\\Sigma$, and $V$ as matrices with shapes $\\left(m, r\\right)$, $\\left(r, r\\right)$, $\\left(r, n\\right)$.\n",
    "\n",
    "Sometimes, we will choose the former one to be consistent with what is adopted by `numpy`.\n",
    "\n",
    "At other times, we'll use the latter convention in which $\\Sigma$ is an $r \\times r$  diagonal matrix.\n",
    "\n",
    "Also, when we discuss the **dynamic mode decomposition** below, we'll use a special case of the latter  convention in which it is understood that\n",
    "$r$ is just a pre-specified small number of leading singular values that we think capture the  most interesting  dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digression:** Through  the following identities, the singular value decomposition (SVD) is related to the **polar decomposition** of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} X & = SQ  \\cr  \n",
    "    S & = U\\Sigma U^T \\cr\n",
    "    Q & = U V^T \n",
    "\\end{align*}\n",
    "\n",
    "where  $S$ is evidently a symmetric matrix and $Q$ is an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA \n",
    "\n",
    "Let's begin with the case in which $n >> m$, so that we have many  more observations $n$ than random variables $m$.\n",
    "\n",
    "\n",
    "The data matrix $X$ is **short and fat**  in the  $n >> m$ case as opposed to a **tall and skinny** case with $m > > n $ to be discussed later in this notebook.\n",
    "\n",
    "\n",
    "We regard  $X$ as an  $m \\times n$ matrix of **data**:\n",
    "\n",
    "$$ X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix} $$\n",
    "\n",
    "where for $j = 1, \\ldots, n$ the column vector $ X_j = \\begin{bmatrix}X_{1j}\\\\X_{2j}\\\\\\vdots\\\\X_{mj}\\end{bmatrix}$ is a  vector of observations on variables $  \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{bmatrix}$.\n",
    "\n",
    "In a **time series** setting, we would think of columns $j $ as indexing different __times__ at which random variables are observed, while rows index different random variables.\n",
    "\n",
    "\n",
    "In a **cross section** setting, we would think of columns $j $ as indexing different __individuals__ for  which random variables are observed, while rows index different random variables.\n",
    "\n",
    "The number of singular values equals the rank of  matrix $X$.\n",
    "\n",
    "Arrange the singular values  in decreasing order.\n",
    "\n",
    "Arrange   the positive singular values on the main diagonal of the matrix $\\Sigma$ of   into a  vector $\\sigma_R$.\n",
    "\n",
    "Set all other entries of $\\Sigma$ to zero.\n",
    "\n",
    "\n",
    "#### PCA and SVD\n",
    "\n",
    "To relate a SVD to a PCA (principal component analysis) of data set $X$, first construct  the  SVD of the data matrix $X$:\n",
    "\n",
    "$$X = U \\Sigma V^T = \\sigma_1 U_1 V_1^T + \\sigma_2 U_2 V_2^T + \\cdots + \\sigma_r U_r V_r^T \n",
    "\\label{eq:PCA1} \\tag{1} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}$$\n",
    "\n",
    "$$V^T = \\begin{bmatrix}V_1^T\\\\V_2^T\\\\\\ldots\\\\V_n^T\\end{bmatrix}$$\n",
    "\n",
    "In equation \\eqref{eq:PCA1}, each of the $m \\times n$ matrices $U_{j}V_{j}^T$ is evidently\n",
    "of rank $1$. \n",
    "\n",
    "\n",
    "Thus, we have \n",
    "\n",
    "$$ X = \\sigma_1 \\begin{pmatrix}U_{11}V_{1}^T\\\\U_{21}V_{1}^T\\\\\\cdots\\\\U_{m1}V_{1}^T\\\\\\end{pmatrix} + \\sigma_2\\begin{pmatrix}U_{12}V_{2}^T\\\\U_{22}V_{2}^T\\\\\\cdots\\\\U_{m2}V_{2}^T\\\\\\end{pmatrix}+\\ldots + \\sigma_r\\begin{pmatrix}U_{1r}V_{r}^T\\\\U_{2r}V_{r}^T\\\\\\cdots\\\\U_{mr}V_{r}^T\\\\\\end{pmatrix}\n",
    "\\label{eq:PCA2} \\tag{2} $$\n",
    " \n",
    "\n",
    "Here is how we would interpret the objects in the  matrix equation \\eqref{eq:PCA2} in \n",
    "a time series context:\n",
    "\n",
    "* $ V_{k}^T= \\begin{bmatrix}V_{k1} &  V_{k2} & \\ldots & V_{kn}\\end{bmatrix}  \\quad \\textrm{for each} \\   k=1, \\ldots, n $ is a time series  $\\lbrace V_{kj} \\rbrace_{j=1}^n$ for the $k$th principal component\n",
    "\n",
    "* $U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m$\n",
    "is a vector of loadings of variables $X_i$ on the $k$th principle component,  $i=1, \\ldots, m$\n",
    "\n",
    "* $\\sigma_k $ for each $k=1, \\ldots, r$ is the strength of $k$th **principal component**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: reduced (or economy) versus full SVD\n",
    "\n",
    "You can read about reduced and full SVD here\n",
    " <https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html>\n",
    " \n",
    "Let's do a small experiment to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U, S, V =\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[-0.28241195, -0.17748247, -0.53207936, -0.77117071, -0.10455027],\n",
       "        [-0.46417787, -0.04950694, -0.24034722,  0.44552211, -0.72514222],\n",
       "        [-0.56734263,  0.05321377,  0.7548165 , -0.3139495 , -0.08353678],\n",
       "        [-0.40724396,  0.78272425, -0.27531398,  0.10937375,  0.36569754],\n",
       "        [-0.46589387, -0.59208114, -0.11652911,  0.31028839,  0.56791277]]),\n",
       " array([1.50726289, 0.58317586]),\n",
       " array([[-0.64829609, -0.76138832],\n",
       "        [ 0.76138832, -0.64829609]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(5,2)\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print('U, S, V ='), U, S, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uhat, Shat, Vhat = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[-0.28241195, -0.17748247],\n",
       "        [-0.46417787, -0.04950694],\n",
       "        [-0.56734263,  0.05321377],\n",
       "        [-0.40724396,  0.78272425],\n",
       "        [-0.46589387, -0.59208114]]),\n",
       " array([1.50726289, 0.58317586]),\n",
       " array([[-0.64829609, -0.76138832],\n",
       "        [ 0.76138832, -0.64829609]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uhat, Shat, Vhat = '), Uhat, Shat, Vhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = np.linalg.matrix_rank(X)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "Add some words about the \"economy SVD\" and add an example here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with eigenvalues and eigenvectors\n",
    "\n",
    "We now  turn to using the eigen decomposition of a sample  covariance matrix to do PCA.\n",
    "\n",
    "Let $X_{m \\times n}$ be our $m \\times n$ data matrix.\n",
    "\n",
    "Let's assume that sample means of all variables are zero.\n",
    "\n",
    "We can make sure that this is true by **pre-processing** the data by substracting sample means appropriately.\n",
    "\n",
    "Define the sample covariance matrix $\\Omega$ as \n",
    "\n",
    "$$\\Omega = XX^T $$\n",
    "\n",
    "Then use an eigen decomposition to represent $\\Omega$ as follows:\n",
    "\n",
    "$$ \\Omega =P\\Lambda P^T$$\n",
    "\n",
    "Here \n",
    "\n",
    "* $P$ is $mÃ—m$ matrix of eigenvectors of $\\Omega$\n",
    "\n",
    "* $\\Lambda$ is a diagonal matrix of eigenvalues of $\\Omega$\n",
    "\n",
    "We can then represent $X$ as\n",
    "\n",
    "$$X=P\\epsilon $$          \n",
    "\n",
    "where \n",
    "\n",
    "$$\\epsilon\\epsilon^T=\\Lambda $$ \n",
    "\n",
    "We can verify that\n",
    "\n",
    "$$XX^T=P\\Lambda P^T$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows that we can represent the data matrix as \n",
    "\n",
    "\n",
    "\\begin{equation*} X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix}\n",
    "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
    "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m \\end{equation*}\n",
    "\n",
    "where \n",
    "\n",
    "$$  \\epsilon\\epsilon^T=\\Lambda $$\n",
    "\n",
    "To reconcile the preceding representation with the PCA that we obtained through the SVD above, we first note that $\\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j$.\n",
    "\n",
    "Now define  $\\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}}$\n",
    "which evidently implies that $\\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^T=1$.\n",
    "\n",
    "Therefore \n",
    "\n",
    "\n",
    "$\\begin{aligned}\n",
    "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\\n",
    "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m}\n",
    "\\end{aligned}$\n",
    "\n",
    "which evidently agrees with \n",
    "\n",
    "$X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T}$\n",
    "\n",
    "provided that  we set \n",
    "\n",
    "* $U_j=P_j$ (the loadings of variables on principal components) \n",
    "\n",
    "* ${V_k}^{T}=\\tilde{\\epsilon_k}$ (the principal components)\n",
    "\n",
    "Since there are several possible ways of computing  $P$ and $U$ for  given a data matrix $X$, depending on  algorithms used, we might have sign differences or different orders between eigenvectors.\n",
    "\n",
    "We want a way that leads to the same $U$ and $P$. \n",
    "\n",
    "In the following, we accomplish this by\n",
    "\n",
    "1. sorting   eigenvalues and singular values in descending order\n",
    "2. imposing positive diagonals on $P$ and $U$ and adjusting signs in $V^T$ accordingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections\n",
    "\n",
    "To pull things together, it is useful to assemble and compare some formulas presented above.\n",
    "\n",
    "First, consider the following SVD of an $m \\times n$ matrix:\n",
    "$$ X = U\\Sigma V^T $$\n",
    "\n",
    "\n",
    "Compute\n",
    "\n",
    "\\begin{align*}\n",
    "   XX^T&=U\\Sigma V^TV\\Sigma^T U^T\\cr\n",
    "  &\\equiv U\\Sigma\\Sigma^TU^T\\cr\n",
    "  &\\equiv U\\Lambda U^T\n",
    "   \\end{align*}\n",
    "         \n",
    "         \n",
    "Thus, $U$ in the SVD is the matrix $P$  of\n",
    "eigenvectors of $XX^T$ and $\\Sigma \\Sigma^T$ is the matrix $\\Lambda$ of eigenvalues.\n",
    "\n",
    "Second, let's compute\n",
    "\n",
    "\\begin{align*}\n",
    "         X^TX &=V\\Sigma^T U^TU\\Sigma V^T\\\\\n",
    "            &=V\\Sigma^T{\\Sigma}V^T\n",
    "         \\end{align*} \n",
    "\n",
    "Thus, the matrix  $V$ in the SVD is the matrix of  eigenvectors of  $X^TX$\n",
    "\n",
    "\n",
    "Summarizing and fitting things together, we have the eigen decomposition of the sample\n",
    "covariance matrix\n",
    "\n",
    "$$ X X^T = P \\Lambda P^T $$\n",
    "\n",
    "where $P$ is an orthogonal matrix.\n",
    "\n",
    "Further, from the SVD of $X$, we know that\n",
    "\n",
    "$$ X X^T = U \\Sigma \\Sigma^T U^T $$\n",
    "\n",
    "where $U$ is an orthonal matrix.  \n",
    "\n",
    "Thus, $P = U$ and we have the representation of $X$\n",
    "\n",
    "$$ X = P \\epsilon = U \\Sigma V^T  $$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$ U^T X = \\Sigma V^T = \\epsilon . $$\n",
    "\n",
    "\n",
    "Note that the preceding implies that\n",
    "\n",
    "$$ \\epsilon \\epsilon^T = \\Sigma V^T V \\Sigma^T = \\Sigma \\Sigma^T = \\Lambda ,$$\n",
    "\n",
    "so that everything fits together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a class `DecomAnalysis` that wraps  PCA and SVD for a given a data matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomAnalysis:\n",
    "    \"\"\"\n",
    "    A class for conducting PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, n_component=None):\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        self.Î© = (X @ X.T)\n",
    "\n",
    "        self.m, self.n = X.shape\n",
    "        self.r = LA.matrix_rank(X)\n",
    "\n",
    "        if n_component:\n",
    "            self.n_component = n_component\n",
    "        else:\n",
    "            self.n_component = self.m\n",
    "\n",
    "    def pca(self):\n",
    "\n",
    "        ðœ†, P = LA.eigh(self.Î©)    # columns of P are eigenvectors\n",
    "\n",
    "        ind = sorted(range(ðœ†.size), key=lambda x: ðœ†[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        self.ðœ† = ðœ†[ind]\n",
    "        P = P[:, ind]\n",
    "        self.P = P @ diag_sign(P)\n",
    "\n",
    "        self.Î› = np.diag(self.ðœ†)\n",
    "\n",
    "        self.explained_ratio_pca = np.cumsum(self.ðœ†) / self.ðœ†.sum()\n",
    "\n",
    "        # compute the N by T matrix of principal components \n",
    "        self.ðœ– = self.P.T @ self.X\n",
    "\n",
    "        P = self.P[:, :self.n_component]\n",
    "        ðœ– = self.ðœ–[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "    def svd(self):\n",
    "\n",
    "        U, ðœŽ, VT = LA.svd(self.X)\n",
    "\n",
    "        ind = sorted(range(ðœŽ.size), key=lambda x: ðœŽ[x], reverse=True)\n",
    "\n",
    "        # sort by eigenvalues\n",
    "        d = min(self.m, self.n)\n",
    "\n",
    "        self.ðœŽ = ðœŽ[ind]\n",
    "        U = U[:, ind]\n",
    "        D = diag_sign(U)\n",
    "        self.U = U @ D\n",
    "        VT[:d, :] = D @ VT[ind, :]\n",
    "        self.VT = VT\n",
    "\n",
    "        self.Î£ = np.zeros((self.m, self.n))\n",
    "        self.Î£[:d, :d] = np.diag(self.ðœŽ)\n",
    "\n",
    "        ðœŽ_sq = self.ðœŽ ** 2\n",
    "        self.explained_ratio_svd = np.cumsum(ðœŽ_sq) / ðœŽ_sq.sum()\n",
    "\n",
    "        # slicing matrices by the number of components to use\n",
    "        U = self.U[:, :self.n_component]\n",
    "        Î£ = self.Î£[:self.n_component, :self.n_component]\n",
    "        VT = self.VT[:self.n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "    def fit(self, n_component):\n",
    "\n",
    "        # pca\n",
    "        P = self.P[:, :n_component]\n",
    "        ðœ– = self.ðœ–[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_pca = P @ ðœ–\n",
    "\n",
    "        # svd\n",
    "        U = self.U[:, :n_component]\n",
    "        Î£ = self.Î£[:n_component, :n_component]\n",
    "        VT = self.VT[:n_component, :]\n",
    "\n",
    "        # transform data\n",
    "        self.X_svd = U @ Î£ @ VT\n",
    "\n",
    "def diag_sign(A):\n",
    "    \"Compute the signs of the diagonal of matrix A\"\n",
    "\n",
    "    D = np.diag(np.sign(np.diag(A)))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that prints out information so that we can compare  decompositions\n",
    "obtained by different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pca_svd(da):\n",
    "    \"\"\"\n",
    "    Compare the outcomes of PCA and SVD.\n",
    "    \"\"\"\n",
    "\n",
    "    da.pca()\n",
    "    da.svd()\n",
    "\n",
    "    print('Eigenvalues and Singular values\\n')\n",
    "    print(f'Î» = {da.Î»}\\n')\n",
    "    print(f'Ïƒ^2 = {da.Ïƒ**2}\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    # loading matrices\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('loadings')\n",
    "    axs[0].plot(da.P.T)\n",
    "    axs[0].set_title('P')\n",
    "    axs[0].set_xlabel('m')\n",
    "    axs[1].plot(da.U.T)\n",
    "    axs[1].set_title('U')\n",
    "    axs[1].set_xlabel('m')\n",
    "    plt.show()\n",
    "\n",
    "    # principal components\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    plt.suptitle('principal components')\n",
    "    axs[0].plot(da.Îµ.T)\n",
    "    axs[0].set_title('Îµ')\n",
    "    axs[0].set_xlabel('n')\n",
    "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.Î»))\n",
    "    axs[1].set_title('$V^T*\\sqrt{\\lambda}$')\n",
    "    axs[1].set_xlabel('n')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dynamic Mode Decomposition (DMD)**\n",
    "\n",
    "We now turn to the case in which $m >>n $ so that there are many more random variables $m$ than observations $n$.\n",
    "\n",
    "This is the **tall  and skinny** case associated with **Dynamic Mode Decomposition**.\n",
    "\n",
    "Starting with an $m \\times n $ matrix of data $X$, we form two matrices \n",
    "\n",
    "$$ \\tilde X =  \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_{n-1}\\end{bmatrix} $$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ \\tilde X' =  \\begin{bmatrix} X_2 \\mid X_3 \\mid \\cdots \\mid X_n\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "In forming $\\tilde X$ and $\\tilde X'$, we have in each case  dropped a column from $X$.\n",
    "\n",
    "Evidently, $\\tilde X$ and $\\tilde X'$ are both $m \\times \\tilde n$ matrices where $\\tilde n = n - 1$.\n",
    "\n",
    "\n",
    "We start with a system consisting of $m$  least squares regressions of *everything on everything*:\n",
    "\n",
    "$$ \\tilde X' = A \\tilde X + \\epsilon  $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ A = \\tilde X' \\tilde X^{+} $$\n",
    "\n",
    "and where the (huge) $m \\times m $ matrix $X^{+}$ is the Moore-Penrose generalize inverse of $X$ that we could compute\n",
    "as \n",
    "\n",
    "$$ X^{+} = V \\Sigma^{-1} U^T $$\n",
    "\n",
    "where the matrix $\\Sigma^{-1}$ is constructed by replacing each non-zero element of $\\Sigma$ with $\\sigma_j^{-1}$.\n",
    "\n",
    "\n",
    "The idea behind **dynamic mode decomposition** is to construct an approximation that  \n",
    "\n",
    " * sidesteps computing  $X^{+}$\n",
    " \n",
    " * retains only the largest  $\\tilde r< < r$ eigenvalues and associated eigenvectors of $U$ and $V^T$ \n",
    " \n",
    " * constructs an $m \\times \\tilde r$ matrix $\\Phi$ that captures effects of $r$ dynamic modes on all $m$ variables\n",
    " \n",
    " * uses $\\Phi$ and the $\\tilde r$ leading singular values to forecast *future* $X_t$'s\n",
    "\n",
    "\n",
    "\n",
    "The magic of **dynamic mode decomposition** is that we accomplish this without ever computing the regression coefficients $A = X' X^{+}$.\n",
    "\n",
    "To accomplish a DMD, we deploy the following steps:\n",
    "\n",
    " * Compute the singular value decomposition \n",
    " \n",
    "     $$ X = U \\Sigma V^T  $$\n",
    "    \n",
    "    where $U$ is $m \\times r$, $\\Sigma$ is an $r \\times r$ diagonal  matrix, and $V^T$ is an $r \\times \\tilde n$ matrix. \n",
    "    \n",
    "    \n",
    "  * Notice that (though it would be costly), we could compute $A$ by solving \n",
    "  \n",
    "    $$ A = X' V \\Sigma^{-1} U^T $$\n",
    "    \n",
    "    But we won't do that.  \n",
    "    \n",
    "    Instead we'll proceed as follows.\n",
    "    \n",
    "    Note that since,  $X' = A U \\Sigma V^T$, we know that \n",
    "    \n",
    "    $$ A U  =  X' V \\Sigma^{-1} $$\n",
    "     \n",
    "    so that \n",
    "    \n",
    "     $$ U^T X' V \\Sigma^{-1} = U^T A U \\equiv \\tilde A $$\n",
    "     \n",
    "  * At this point,  in constructing $\\tilde A$ according to the above formula,\n",
    "    we take only the  columns of $U$ corresponding to the $\\tilde r$ largest singular values.  \n",
    "    \n",
    "        \n",
    "    Tu et al. verify that eigenvalues and eigenvectors of $\\tilde A$ equal the leading eigenvalues and associated eigenvectors of $A$.\n",
    "  \n",
    "  * Construct an eigencomposition of $\\tilde A$ that satisfies\n",
    "  \n",
    "    $$ \\tilde A W =  W \\Lambda $$\n",
    "    \n",
    "    where $\\Lambda$ is a $\\tilde r \\times \\tilde r$ diagonal matrix of eigenvalues and the columns of $W$ are corresponding eigenvectors\n",
    "    of $\\tilde A$.\n",
    "    Both $\\Lambda$ and $W$ are $\\tilde r \\times \\tilde r$ matrices.\n",
    "    \n",
    "  * Construct the $m \\times \\tilde r$ matrix\n",
    "  \n",
    "    $$ \\Phi = X' V \\Sigma^{-1} W $$\n",
    "    \n",
    "    Let $\\Phi^{+}$ be a generalized inverse of $\\Phi$; $\\Phi^{+}$ is an $\\tilde r \\times m$ matrix. \n",
    "    \n",
    "  * Define  an  initial  vector $b$ of dominant modes by\n",
    "  \n",
    "    $$ b= \\Phi^{+} X_1 $$\n",
    "    \n",
    "    where evidently $b$ is an $\\tilde r \\times 1$ vector.\n",
    "    \n",
    "With $\\Lambda, \\Phi$ in hand, our least-squares fitted dynamics fitted to the $r$ dominant modes\n",
    "are governed by\n",
    "\n",
    "$$ X_{t+1} = \\Phi \\Lambda \\Phi^{+} X_t $$\n",
    "\n",
    " \n",
    "Conditional on $X_t$, forecasts $\\check X_{t+j} $ of $X_{t+j}, j = 1, 2, \\ldots, $  are evidently given by \n",
    "\n",
    " $$\\check X_{t+j} = \\Phi \\Lambda^j \\Phi^{+} X_t  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source for some Python code\n",
    "\n",
    "You can find a Python implementation of DMD here:\n",
    "\n",
    "https://mathlab.github.io/PyDMD/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
